{"source":"cat request.json","lang":14,"metadata_sources":[],"sub_type":"","precision_check":1.0e+100,"testcases":["x"],"custom":true,"checker_program":"\n# Start of HEAD\nfrom __future__ import division, print_function, unicode_literals, absolute_import\nimport json\nimport string\nimport sys\n# End of HEAD\n# Start of BODY\n'''\nTestStruct::\ntestcase_id                   [int] ID of the test-case\ntestcase_input_path           [str] File path to test-case input\ntestcase_output_path          [str] File path to test-case output generated by the problem solver\ntestcase_expected_output_path [str] File path to test-case expected output to be matched with\nmetadata_file_paths           [list\u003cstr\u003e] File paths to Question metadata (Extra files usually used for defining traning sets)\nsubmission_code_path          [str] File path to submission source code\ntestcase_result               [bool] Set to True if test-case output matches test-case expected output. Matching is done line by line\ntestcase_signal               [int] Exit code of the test-case process\ntestcase_time                 [float] Time taken by the test-case process in seconds\ntestcase_memory               [int] Peak memory of the test-case process determined in bytes\ndata                          [str] \u003cFuture use\u003e\nResultStruct::\nresult      [bool]  Assign test-case result. True determines success. False determines failure\nscore       [float] Assign test-case score. Normalized between 0 to 1\nmessage     [str] Assign test-case message. This message is visible to the problem solver\n'''\n\ndef ERR(obj, msg = \"\"):\n    obj.result = False\n    obj.score = 0\n    obj.message = 'Error reading result file.' + msg\n\ndef WA(obj, score = 0.0):\n    \"\"\"\n    Populates given object with data indicating Wrong Answer result\n    \"\"\"\n    obj.result = False\n    obj.score = score\n    obj.message = \"Wrong Answer\"\n    obj.testcase_signal = 99\n    \ndef TLE(obj, score = 0.0):\n    obj.result = False\n    obj.score = score\n    obj.message += \"Time Limit Exceeded\"\n    obj.testcase_signal = 10\n        \ndef ACC(obj, score = 1.0):\n    \"\"\"\n    Populates given object with data indicating Accepted result\n    \"\"\"\n    obj.result = True\n    obj.score = score\n    obj.message += \"Success\\n\"\n\n\n\ndef solve(k, a, d):\n    p = 0\n    n = 0\n    z = 0\n    \n    for x in a:\n        if x \u003c -1000 or x \u003e 1000:\n            return 0\n        \n        if x == 0:\n            z += 1\n        if x \u003e 0:\n            p += 1\n        if x \u003c 0:\n            n += 1\n            \n    if p == 0 or n == 0 or z == 0:\n        return 0\n    \n    r = \"\"\n    c = 0\n    \n    for x in a:\n        if x \u003c= 0:\n            c += 1\n    \n    if c \u003e= k:\n        r = \"n\"\n    else:\n        r = \"y\"\n        \n    if d % 2 == 1 and r == \"y\":\n        return 1\n    if d % 2 == 0 and r == \"n\":\n        return 1\n    \n    return 0\n\ndef run_custom_checker(t_obj, r_obj):\n    # Don't print anything to STDOUT in this function\n    # Enter your custom checker scoring logic here\n\n    output_str = ''\n    try:\n        output_str = open(t_obj.testcase_output_path, 'r').read().strip()\n        outputs = output_str.split(\"\\n\")\n        exp_output_str = open(t_obj.testcase_expected_output_path, 'r').read().strip()\n        exp_outputs = exp_output_str.split(\"\\n\")\n        input_str = open(t_obj.testcase_input_path, 'r').read().strip()\n        inputs = input_str.split(\"\\n\")\n    except IOError:\n        ERR(r_obj)\n        return\n    \n    if len(outputs) == 0:\n        return WA(r_obj)\n    l = 0;\n    t = int(outputs[l])\n    \n    if 2 * t + 1 != len(outputs):\n        return WA(r_obj)\n    \n    if t \u003e 5:\n        t = 5\n    \n    l += 1\n    \n    c = 0.0\n    d = 1\n    \n    ns = []\n    \n    while t \u003e 0:\n        n, k = map(int, outputs[l].strip().split(' '))\n        a = map(int, outputs[l + 1].strip().split(' '))\n        \n        if len(a) == n:\n            if (n \u003e= 1 and n \u003c= 200 and k \u003e= 1 and n \u003e= k):\n                if n in ns:\n                    c += 0.0\n                else:\n                    ns.append(n)\n                    c += solve(k, a, d)\n        \n        l += 2\n        d += 1\n        t -= 1\n    \n    return ACC(r_obj, c / 5.0)\n\n# Start of TAIL\nclass TestStruct:\n    def __init__(self):\n        self.testcase_id = 0\n        self.testcase_input_path = \"\"\n        self.testcase_output_path = \"\"\n        self.testcase_expected_output_path = \"\"\n        self.metadata_file_paths = []\n        self.submission_code_path = \"\"\n        self.testcase_result = False\n        self.testcase_signal = 0\n        self.testcase_time = 0.0\n        self.testcase_memory = 0\n        self.data = \"\"\n\n\nclass ResultStruct:\n    def __init__(self):\n        self.result = False\n        self.score = 0.0\n        self.message = \"\"\n\n\ndef read_input_json(json_file_path, t_obj):\n    file_obj = open(json_file_path, \"r\")\n    json_file_contents = file_obj.read()\n\n    root = {}\n    try:\n        root = json.loads(json_file_contents)\n    except ValueError:\n        return 1\n\n    try:\n        # Read values\n        t_obj.testcase_id = root[\"testcase_id\"]\n        t_obj.testcase_input_path = root[\"input_file_path\"]\n        t_obj.testcase_output_path = root[\"output_file_path\"]\n        t_obj.testcase_expected_output_path = root[\"expected_output_file_path\"]\n        t_obj.metadata_file_paths = root[\"metadata_file_paths\"]\n        t_obj.submission_code_path = root[\"submission_code_path\"]\n        t_obj.testcase_result = root[\"testcase_result\"]\n        t_obj.testcase_signal = root[\"testcase_signal\"]\n        t_obj.testcase_time = root[\"testcase_time\"]\n        t_obj.testcase_memory = root[\"testcase_memory\"]\n        t_obj.data = root[\"data\"]\n    except KeyError:\n        return 1\n\n    return 0\n\n\ndef write_result_json(r_obj):\n    root = {\n        \"custom_result\" : int(r_obj.result),\n        \"custom_score\"  : max((r_obj.score if (r_obj.score \u003c= 1.0) else 1.0), 0),\n        \"custom_message\": r_obj.message if (len(r_obj.message) \u003c= 4096) else r_obj.message[0:4095]\n    }\n\n    print(json.dumps(root))\n\n\nif __name__ ==  \"__main__\":\n    # Input parameters\n    t_obj = TestStruct()\n    # Result parameters\n    r_obj = ResultStruct()\n\n    if len(sys.argv) \u003c 2:\n        write_result_json(r_obj)\n        sys.exit(1)\n\n    # Decode input JSON\n    failure = read_input_json(sys.argv[1], t_obj)\n    # Incase input JSON was malformed or not existent\n    if failure != 0:\n        r_obj.message = \"Unable to read input json\";\n        write_result_json(r_obj)\n        sys.exit(2)\n\n    #Run the custom checker evaluator\n    run_custom_checker(t_obj, r_obj)\n\n    # Encode result JSON\n    write_result_json(r_obj)\n    sys.exit(0)\n# End of TAIL\n        ","checker_lang":5,"custom_checker_version":"v2","callback_url":"https://internal-callback.hackerrank.com/codechecker/testcallback/383222031","checkerlimits":{},"callback_params":{"http":["stderr","diff_status","stdout","stdout_debug","censored_stderr","error_markers"],"s3":[]},"maximum_stdout_size":102400,"analytics":{"interface":"hackerrank","mode":"Compile-Test"},"hash":"hackerrank-9cb4ba826548257cf1ffb57fe8308c69","request_s3_path":"7ae4929b6ba7-2018_04_14_19/hackerrank-90582d4bee727113b717e432e3f5c083"}